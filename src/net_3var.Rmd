---
bibliography: citation.bib
csl: elsevier-vancouver.csl
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::pdf_document2:
    includes:
      in_header: Templates/preamble.tex
    keep_tex: yes
    number_sections: yes
    toc: yes
  html_document: default
---

## Inferring a three variable Bayesian network

I reparametrise the joint probability of the graph by replacing the conditional probability $P(child|parent)$ to be the quotient of two joint distirbution $\frac{P(child,parent)}{P(parent)}$. Because $P(child|parent)$ enters the marginalised likelihood as a Beta-Binomial probability,  $P(parent)$ enters the term as a Drichlet-Multinomial probability:

$$
P(x_k)={\frac {\left(n!\right)\Gamma \left(\sum \alpha _{k}\right)}{\Gamma \left(n+\sum \alpha _{k}\right)}}\prod _{k=1}^{K}{\frac {\Gamma (x_{k}+\alpha _{k})}{\left(x_{k}!\right)\Gamma (\alpha _{k})}}
$$
where $\sum{x_k}=N$ is the partition of sample into k categories, $\alpha_k$ is the imaginary sample size for each category (also known as prior concentration). This formulation has the advantage of easier coding. 

To be consistent with bnlearn and deal, I did discared the multinomial terms in the calculation, leading to 

$$
P(x_k)={\frac {\Gamma \left(\sum \alpha _{k}\right)}{\Gamma \left(n+\sum \alpha _{k}\right)}}\prod _{k=1}^{K}{\frac {\Gamma (x_{k}+\alpha _{k})}{\Gamma (\alpha _{k})}}
$$


## Number of Bayesian networks:

A V-variable network has $V(V-1)/2$ bivariate interaction (edges), each interaction can have 3 possible status (A->B, A<-B, A B). Hence altogether there are $n(V) = 3^{V(V-1)/2}$ possible networks. For $V=3$,$n(3)=27$

However, for this exerecise, the serach space is restircted to the graph set $G=$ {no-edge, A-C only, A-C and B-C}.


```{r,echo = F,results='hide'}
knitr::opts_chunk$set(
  echo = F
 # ,results = 'hide' ##### Will hide tables
 ,cache = F
 ,eval = T
 ,eval.after = c('fig.cap','code')
)
suppressMessages({
  invisible({
  source('dag.R')
  source('plot_dag.R')
  })
})
```

![Graph with no edge](graph_0edge.jpg)
![Graph with an edge between A and C](graph_1edge.jpg)

![Graph with two edges but not A-B](graph_2edge.jpg){height=50%}

```{r,fig.cap = cap,fig.show='hide',results='hide'}
cap=''
dfs <- list()
df <- main(dat1,iss=8)
dfs <- rbind(dfs,df)
df <- main(dat1,iss=1)
dfs <- rbind(dfs,df)
bn = bnlearn::pc.stable(dat1)
png('pcalgo_dat1.png',height = 3,width=4.5,units = 'in',res=200)
par(mar=c(0,0,0,0))
plot(bn)
dev.off()
plot(bn)
```


```{r,height = 3,width=3,fig.show='hide',results='hide'}
# dat = dat2
df <- main(dat2,iss=8)
dfs <- rbind(dfs,df)
df <- main(dat2,iss=1)
dfs <- rbind(dfs,df)
bn = bnlearn::pc.stable(dat2)
png('pcalgo_dat2.png',height = 3,width=4.5,units = 'in',res=200)
# dev.copy(dev.prev)
par(mar=c(0,0,0,0))
plot(bn)
dev.off()
plot(bn)
```

\begin{figure}
\includegraphics[width=0.45\textwidth]{pcalgo_dat1.png}
\includegraphics[width=0.45\textwidth]{pcalgo_dat2.png}
\caption{ \label{fig:pc-best}Best networks inferred using "bnlearn::pc.stable". Left: Dataset1. Right: Dataset2}
\end{figure}


\iffalse
\includegraphics[page=1,width=\paperwidth]{popgen_eqn_p3.pdf}
\fi

### Comment on the likelihood-equivalent prior

The likelihood-equivalent prior is set so that the imaginary sample size decreases as data is stratified by more variables. For example, if $P(A=1) \sim Beta(\eta(A_0),\eta(A_1))$, then the imaginary sample size for (A=1) is $\eta(A_1)=2$. Hence if we then ask for $P(B=1\gvn A=1)\sim Beta(\eta(B_0A_1),\eta(B_1A_1))$, the imaginary $\eta$'s must add up to the imageinary sample size of the condition $\gvn A=1$, (aka $\eta(B_0A_1)+\eta(B_1A_1)=\eta(A_1)=2$). Assuming two events are equally probable gives $\eta(B_0A_1)=\eta(B_1A_1)=1$. For 3 variable, we can deduce $8\eta(ABC)=4\eta(AB)=2\eta(A)=\eta(0)$, setting $\eta(ABC)=1$ gives $\eta(ABC)=1,\eta(AB)=2,\eta(A)=4,\eta(0)=8$, corresponding to different levels of stratification. 

If a likelihood-preserving prior is used, then it is only the correlation structure that determines the relative feasibility of different graphs. Consider the 1-edge and 0-edge examples, the 0-edge example asserts 
$P(A\gvn C=0)=P(A\gvn C=1)=P(A)$, whereas the 1-edge example implies $P(A\gvn C=0)\neq P(A\gvn C=1)$, allowing an additional degree of freedom. The striking fact is that this additional DOF does not necessarily leads to a better model, in constrast to conventional mixture models where additional components always reduce likelihood. One of the reason is that the partiaion of $(A_0)=(A_0C_0)+(A_0C_1)$ is not arbitratry, but the general case is still confusing. A possible intution is that the additional DOF project the paramteric space to a higher dimension where the likelihood function overlaps less with the prior distribution.

### Drawbacks of binary bayesian networks

If there are hidden latent variables in the bayes net, for example where the common parent of A and B (which is C) is conceived from the observers, then one will have to consider a graph with hidden variable in order to explain the data. In other words, a graphical prior needs to accommodate additional nodes to explain such data. Even though this is the case, it will be hard to express the case where n(A_0)=n(B_0)

### Effect of imaginary sample size

Here we consider two imaginary sample sizes $\eta(0)=8$ and $\eta(0)=1$. A higher $\eta$ indicates a sharper distribution of binomial probability $\theta$ (Setting $\eta(0)=1$ implies $\eta(ABC)=0.125,\eta(AB)=0.25,\eta(A)=0.5,\eta(0)=1$)

The corresponding likelihood are calculated for both datasets (dat1 and dat2, see table \ref{tab:likelihood}). 

  - For dat1, the chain network (A-B-C) is the best at ISS=1, the A-B..C network is the best at ISS=8.

  - For dat2, the chain network (A-B-C) is the best for ISS=1 and ISS=8
  
The prediction made by pc.stable is somewhat different (figure \ref{fig:pc-best})

```{r c-gvn-a-idep, fig.cap=cap,fig.height=3,fig.width=8}
cap = 'P(C|A)=P(C) according to the 0-edge graph, Left: P(C|A=0). Right: P(C|A=1)' 
aM <- all.graphs[[1]]
sess <- .PGM_binary$new(mdlgraph=igraph::graph_from_adjacency_matrix(aM))
sess$preprocess()

par(mfrow=c(1,2))
par(mar=c(4.2,3.1,1.1,2.1),
     omi=c(0,0,0,1))

{
  sess$dat <-dat1
  sess$make_table()
  out <- stratifier(sess,3,parent_state = 1)
  ob <- sess$net_posterior(3,parent_state = c(1),debug=0)
  plot_bayes(ob)
  title(bquote(beta(.(out$eta.marg[1]),.(out$eta.marg[2]))~~
                    (N[0]~','~N[1])==(.(out$tb.marg[1])~','~.(out$tb.marg[2]))
                    ))
}


{
  sess$dat <-dat1
  sess$make_table()
  out <- stratifier(sess,3,parent_state = 2)
  ob <- sess$net_posterior(3,parent_state = c(2),debug=0)
  plot_bayes(ob)
  title(bquote(beta(.(out$eta.marg[1]),.(out$eta.marg[2]))~~
                    (N[0]~','~N[1])==(.(out$tb.marg[1])~','~.(out$tb.marg[2]))
                    ))
}
```

```{r c-gvn-a, fig.cap=cap,fig.height=3,fig.width=8}
cap = 'P(C|A) needs to be stratified according to the 1-edge graph, Left: P(C|A=0). Right: P(C|A=1)' 
aM <- all.graphs[[2]]
sess <- .PGM_binary$new(mdlgraph=igraph::graph_from_adjacency_matrix(aM))
sess$preprocess()

par(mfrow=c(1,2))
par(mar=c(4.2,3.1,1.1,2.1),
     omi=c(0,0,0,1))

{
  sess$dat <-dat1
  sess$make_table()
  out <- stratifier(sess,3,parent_state = 1)
  ob <- sess$net_posterior(3,parent_state = c(1),debug=0)
  plot_bayes(ob)
  title(bquote(beta(.(out$eta.marg[1]),.(out$eta.marg[2]))~~
                    (N[0]~','~N[1])==(.(out$tb.marg[1])~','~.(out$tb.marg[2]))
                    ))
}


{
  sess$dat <-dat1
  sess$make_table()
  out <- stratifier(sess,3,parent_state = 2)
  ob <- sess$net_posterior(3,parent_state = c(2),debug=0)
  plot_bayes(ob)
  title(bquote(beta(.(out$eta.marg[1]),.(out$eta.marg[2]))~~
                    (N[0]~','~N[1])==(.(out$tb.marg[1])~','~.(out$tb.marg[2]))
                    ))
}
```

### Plot posteiror for P(C|A) in different models

Here I visulise the posterior distribution using dataset 1 only. In order to show how different graphical models lead to different likelihood, I chose to contrast P(C|A) between \text{[A][B][C]}(0-edge model) and \text{[A][B][C|A]} (1-edge model)

In 0-edge model, $P(C|A)=P(C)$ and the distribution is indifferent for $A=0$ and $A=1$ (figure \ref{fig:c-gvn-a-idep}). The term enters likelihood function as a beta-binomial.

In contrast, the 1-edge model prescribes that $P(A,C)\neq P(A)P(C)$, and two separate distribution must be considered for $P(C|A)$ (figure \ref{fig:c-gvn-a}). The $\prod_{C,A} P(C|A)$ term factors out to be $\prod_{C|A=0} P(C|A=0)\prod_{C|A=0} P(C|A=1)$, as the product of two beta-binomial with independent probability but the same prior. It would be interesting to explore the precise condition under which the factored likelihood exceeds the original single beta-binomial. Clearly, the 0-edge model fails to capture the difference between $P(C|A=0)$ and $P(C|A=1)$ (loglik=-194.3, compared to 1-edge loglik=-175.5), but the underlying mathematics remains to be dissected.

```{r likelihood}
options(digits = 6)
# bn <- bn.list[[2]]
# aMs
# plot(bn)
# bnlearn::modelstring(bn.list[[2]])
knitr::kable(dfs,
             caption = 'Marginalised likelihood of different network topology')
```
