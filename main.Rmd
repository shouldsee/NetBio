---
output:
  # pdf_document: 
  bookdown::pdf_document2:
    keep_tex: yes
    toc: true
    number_sections: yes
    includes: 
      in_header: preamble.tex
  html_document: default
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: citation.bib
csl: elsevier-vancouver.csl
---

`r if(!exists('TITLE')){TITLE = 'test'}`

---
title: `r TITLE`
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = F
 # ,results = 'hide' ##### Will hide tables
 ,cache = F
 ,eval = T
 ,eval.after = c('fig.cap','code')
 )

options(stringsAsFactors = F)  #### Always disable this!
# options(warn=  -1)

```


```{r dependency,eval = F}
PKG = c('Metrics','MLmetrics')
install.packages(PKG)
```


```{r shared_util, include= F}
read_wrap <-function(fname,wid = 100){
  s= readLines(fname)
  s = gsub('\t','  ',s)
  
  # sout = 
  idx = nchar(s) > wid
  
  slst <- as.list(s)
  # browser()
  slst[idx]<-lapply(slst[idx],function(x)strwrap(x,wid))
  unlist(slst)
  
}
```

# Main

## Finding priors for the beta distribution

Beta distribution is the natural prior for a binomial/bernoulli distribution. Considering distribution of a binomial variable $X\gvn\theta\sim Binom(\theta)$, in order to make its marginalisd distribution $P(X) = \int P(X\gvn\theta)P(\theta).d\theta$ analytically tractable, one of the choice is to assume $\theta\sim Beta(M,\alpha)$, so that:

$$
\begin{aligned}
P(\theta) &=\frac{x^{\alpha - 1}(1-x)^{M-\alpha-1}}{B(\alpha,M-\alpha)}
\\
E(\theta) &={\alpha\over M }
\end{aligned}
$$

### Constraint 1:

$$
\begin{aligned}
P(\theta\le 0.25) = P(\theta\ge 0.75) =  0.05 \\
P(\theta\le 0.75)=0.95
\end{aligned}
$$

### Constraint 2


$$
\begin{aligned}
\text{argmax}_\theta(P(\theta))=0.4 \\
P(\theta\le 0.3) = 0.1
\end{aligned}
$$

```{r}

partial <- function(f, ...) {
  'Source: https://stackoverflow.com/questions/32173901/how-to-efficiently-partially-apply-a-function-in-r  by josliber '
  l <- list(...)
  function(...) {
    do.call(f, c(l, list(...)))
  }
}

preview<- function(p,xs =seq(0,1,length.out = 100),silent=F
                   ,xlab = 'x',ylab='y'
                    ,...){
  dots = list(...)
  # if (is.null(dots$xlab)){ dots$xlab='x'}
  # if (is.null(dots$ylab)){ dots$ylab='y'}
  ys= p(xs)
  xlab = (substitute(xlab))
  ylab = (substitute(ylab))
  # ys = deparse(substitute(y))
  # xs = deparse(substitute(xs))
  dat <- list(x=xs,y=ys
              ,xlab=xlab
              ,ylab=ylab
              )
  # dat <- list(y=ys)
  if (silent) {return(dat)}
  # with(data,plot())
  do.call(plot,args = c(dat,dots) )
}
a=1;
m=2
p<-function(x) {pbeta(x,a,m-a)}
dat = preview(p,silent = F,xlab = bquote(theta))
# do.call(plot.default,args = dat)
# with(dat,plot(x,y))

pbeta.ma <- function(x,m,a){pbeta(x,a,m-a)}
dbeta.ma <- function(x,m,a){dbeta(x,a,m-a)}

```

```{r, }

loss <- function(param){
  m = param[1]
  a = param[2]
  # p<-function(x) {pbeta(x,a,m-a)}
  x = c(0.25,0.75)
  y_true = c(0.05,0.95) 
  ps <- pbeta(x,a,m-a)
  Metrics::mse(y_true,ps)
  # mean((ps - y_true)^2)
}


out = optim(c(2,1),loss,method = "BFGS",control = list(maxit=300))
out$par <- as.list(setNames(out$par,c('m','a')))
# outf = partial(dbeta.ma,m = out$par$m,a = out$par$a)
outf = partial(pbeta.ma,m = out$par$m,a = out$par$a)
out$p <- outf
out$d <- partial(dbeta.ma,m = out$par$m,a = out$par$a)
res1 <- out

```


```{r}

loss <- function(param,debug = F){
  m = param[1]
  a = param[2]
  # a = max(a,1)
  # m = max(m,2)
  # p<-function(x) {pbeta(x,a,m-a)}
  # x = c(0.25,0.75)
  # y_true = c(0.05,0.95)
  
  p = partial(pbeta.ma,m=m,a=a)
  d = partial(dbeta.ma,m=m,a=a)
  # MODE = min(max((a-1)/(m-2),0),1)
  opout = optim( c(0.4),
               # d,
         {function(x){-d( min(max(x,1E-4),1-1E-4) )}},
         method = 'BFGS',control = list(maxit=500))
  MODE= opout$par

  
  y_true = c( 0.4, 0.1)
  y_pred = c( MODE, p(0.3)) 
  if (debug){
    # print(MODE)
    print(p(0.3))
    cat('y_pred:',y_pred,'\n')
    cat('y_true:',y_true,'\n')
  }
  Metrics::mae(y_true, y_pred)
  # mean((ps - y_true)^2)
}

system.time({
out = optim(c(10,5),loss,
            control = list(maxit=500)
            # ,method = "BFGS"
            )
out$par <- as.list(setNames(out$par,c('m','a')))
# outf = partial(dbeta.ma,m = out$par$m,a = out$par$a)
outf = partial(pbeta.ma,m = out$par$m,a = out$par$a)
out$p <- outf
out$d <- partial(dbeta.ma,m = out$par$m,a = out$par$a)
res2 <- out
print(res2$value)
loss(unlist(res2$par),debug=T)
  
})
```

```{r}
out <- res1

preview(out$p,type='l'
        ,xlab = bquote(theta)
        ,ylab = bquote('P'~(theta))
                      )
abline(h = c(0.95,0.05),col=2,lty=2)
x = c(.25,.75)
y = outf(x)
points(x,y)
text(x,y,sprintf('(%.3f,%.3f)',x,y),adj = -.5*c(1,1))

options(digits = 3)
tl = with(data = out$par,bquote(M == .(m) ~","~ A == .(a) ~","~ loss==.(out$value) ))
title(tl)
grid()
# plot.default
```

```{r}
out <- res2
par(mar=c(4.2,4.1,1.1,4.1))
preview(out$p,type='l'
        ,xlab = quote(theta)
        ,ylab = quote(P(Theta<theta))
                      )
abline(v = 0.4,col=2,lty=2)

par(new=T)
x = c(.3)
y = out$p(x)
points(x,y)
text(x,y,sprintf('(%.3f,%.3f)',x,y),adj = -.5*c(1,1))

abline(h = y,col=2,lty=2)
tl = with(data = out$par,bquote(M == .(m) ~","~ A == .(a) ~","~ loss==.(out$value) ))
title(tl)

par(new=T)

preview(out$d,type='l',ylab='',xlab='',axes = F,
        col=3)
axis(side = 4)
mtext(side= 4,line = 2,quote(P(Theta=theta)))
grid()
```


```{r}

obsS = '011100101101'
obsB <- strsplit(obsS,'')[[1]] =='1'


out <-res2

obj <-list()
##### Prior: 
prior <- out$d
obj$prior <- prior

#### Likelihood: 
logL_maker <- function(obsB){
  N = length(obsB)
  X = sum(obsB)
  function(p1,...) {
    dbinom(x= X,size = N,p1,...)}
  # return( )
  # d(data)
  # pa * obj$prior
}


L = logL_maker(obsB)
logL = partial(L,log = T)

obj$L <- L
obj$logL <- logL

# margL = logL
dpos <- function(x){L(x)*prior(x)}

margL <- function(p1){
  integrate(dpos,0,1)$value
  }
post <- function(p1){ 
  L(p1)*prior(p1)/margL(p1) }

```

```{r bayes,fig.cap = cap}
cap = paste('Inference of posterior distribution on parameter $\\theta$ given mutatble sequence:', obsS)
par(lwd=2)
par(mar=c(4.2,4.1,1.1,4.1))
i = 1
preview(prior,type= 'l'
        ,xlab = quote(theta)
        ,ylab = quote(P)
        ,lty=i
        ,col=i
        ,ylim=c(0,6)
        ,pch = i
        # ,lwd = 2
        )

i = i+1
# par(new = T)
dat = preview(post,type= 'l',silent = T)
do.call(lines,c(dat
        ,lty=i
        ,col=i
        ,pch = i
        # ,lwd = 2
))
# text(labels = c("red line"))

i = i+1

par(new = T)
preview(L,type= 'b',silent = F
        ,lty=i
        ,col=i
        ,pch=i
        # ,lwd = 2

        ,axes = F,xlab='',ylab='')

mtext(side=4,line=2,'Likelihood');axis(side = 4)

grid()
legend(0, .125, 
       legend=c(
         # substitute(bquote(P(theta))),
         # deparse(substitute(P(theta))),
         expression(P(theta)),
         expression(P(theta~'|'~x)),
         expression(P(x~'|'~theta))),
         # "Prior",
         # 'Posterior',"Likelihood"),
       pch =c(NA,NA,3),
       col=1:i,
       lty=1:i, cex=0.8)
# do.call(lines,c(dat,col=2))
# preview(post,type='l')


# outcounts
```

### Basics for Bayesian inference

$$
\begin{aligned}
\text{likelihood}&: f(\theta) = P(x \gvn \theta) \\
\text{prior}&: f(\theta) = P(\theta) \\
\text{posterior}&: f(\theta)  = P(\theta \gvn x) = \frac{  P(x\gvn \theta) P(\theta)  }{P(x) } \\
\text{marginal likelihood} &: P(x) =  \int P(x\gvn \theta) P(\theta).d\theta
\end{aligned}
$$

The observed toss sequence is: `r obsS`

Assume each coin toss is independent from each other, the likelihood of an observed sequence is only dependent on the total number of heads and not the sequence it occurred in. Denoting the coin toss as a sequence $\{x_i\}$ where $x_i \in \{0,1\}$, we have

$$
\begin{aligned}
\text{\#head} = \indicator\{x_i=1\} \\
\text{\#head} \sim Binom(|\{x_i\}|,\theta)
\end{aligned}
$$
Combining with the prior $\theta\sim$ Beta(`r res2$par$a`, `r with(res2$par,m-1)` ), I calculated the marginal likelihood numerically to be $P(x)=$ `r margL(0)` and derived the posterior distribution accordingly (see figure \ref{fig:bayes}).


## Infer a three variable Bayesian network
