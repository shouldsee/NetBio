---
bibliography: citation.bib
csl: elsevier-vancouver.csl
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::pdf_document2:
    includes:
      in_header: Templates/preamble.tex
    keep_tex: yes
    number_sections: yes
    toc: yes
  html_document: default
---

```{r,echo = F,results='hide'}
recalc = F

knitr::opts_chunk$set(
  echo = F
 # ,results = 'hide' ##### Will hide tables
 ,cache = F
 ,eval = T
 ,eval.after = c('fig.cap','code')
)
suppressPackageStartupMessages({
 source('dream5.R')
  library(ggplot2)
})
if(recalc){
  clu = parallel::makeCluster(16)
}
options(digits=3,
        warn=-1)
```

```{r,eval = T,results='hide'}
load.assignment.data()

qc.list <- list()

install.packages.lazy(c("doSNOW",'doParallel','doMPI'))
```


## Comparing DREAM5 competitors

### Method

#### Data

GNW network: I used the pre-simulated data "medium_network.rda" that included the expression levels of `r ngene` genes along with the underlying true network to test the performance of various algorithms. This dataset is generated by GeneNetWeaver (@gnw) hence called GNW dataset.

DREAM5 dataset: The E. coli network in the Dream5 competition has proved to be fittable and hence used here to evaluate the consistence of algorithms.

#### Formalism

I assumed the interaction matrix $A_{ij}$ is symmetric and binary,  and $A_{ij}=1$ if there is interaction between gene i and gene j, otherwise $A_{ij}=0$. The problem then reduce to a binary classification (existence of an undirected edge) and standard confusion matrix may be constructed to evaluate the performance. Specifically, area-under-precision-recall-curve (AUPR) and area-under-the-ROC (AROC) will be plotted for comparision, along with a list of F1 score.


```{r,eval=recalc}
PKGMethod='naive.spearman'
cap=PKGMethod
PKG = strsplit(PKGMethod,'\\.')[[1]][1]
# library(PKG,character.only = T)
res <- abs(cor(expr.dat,method='spearman'))
qc <- pipeline(res)
qc$method =  PKGMethod
qc.list[[PKGMethod]] <- qc
```

```{r, eval=recalc}
PKGMethod='GENIE3'
fig.cap=PKGMethod
PKG = strsplit(PKGMethod,'\\.')[[1]][1]
library(PKG,character.only = T)
library(doParallel)
set.seed(0)
res <- GENIE3(exprMatrix = t(as.matrix(expr.dat)),nCores = 6,verbose = T)
options(error=recover)
qc <- pipeline(res,silent=1)
qc$method =  PKGMethod
qc.list[[PKGMethod]] <- qc
```

```{r,eval=recalc}
PKGMethod='GeneNet'
fig.cap=PKGMethod
PKG = strsplit(PKGMethod,'\\.')[[1]][1]
library(PKG,character.only = T)
pcor.dyn = ggm.estimate.pcor(expr.dat)
df <- network.test.edges(pcor.dyn,direct = T)
pairs = as.matrix(df[,c('node1','node2')])
res <- pair2adj(pairs,genes=genes,is.indexed = T,symmetric = F
                ,fill = -log(df$pval)
                # ,fill = -log(df$pval.dir+0.0001)
                )
qc <- pipeline(res,silent=1)
qc$method =  PKGMethod
qc.list[[PKGMethod]] <- qc
```


```{r,eval=recalc,fig.cap=cap,fig.height=3,results='hide'}
PKGMethod='bnlearn.bde.bootstrap'
cap=PKGMethod
PKG = strsplit(PKGMethod,'\\.')[[1]][1]
library(PKG,character.only = T)
res <- routine.bnlearn.bootstrap(expr.dat,cluster=clu,score='bde')
qc <- pipeline(res)
qc$method =  PKGMethod
qc.list[[PKGMethod]] <- qc
```


```{r,eval=recalc,fig.cap=cap,fig.height=3,results='hide'}
PKGMethod='minet.mrnet'
cap=PKGMethod
PKG = strsplit(PKGMethod,'\\.')[[1]][1]
library(PKG,character.only = T)
res <- routine.mrnet(expr.dat)
qc <- pipeline(res)
qc$method =  PKGMethod
qc.list[[PKGMethod]] <- qc
```

```{r,eval=recalc,fig.cap=cap,fig.height=3,results='hide'}
PKGMethod='xgboost.Gxgb'
cap=PKGMethod
PKG = strsplit(PKGMethod,'\\.')[[1]][1]
library(PKG,character.only = T)

source('xgb.R')
df <- Gxgb.fit(expr.dat,model.dir = 'qc/',silent = 1)
pairs = as.matrix(df[,c('Feature','output')])
VAR <- apply(expr.dat,2,var)
res <- pair2adj(pairs,genes=genes,is.indexed = F,symmetric = F,
                fill = df$Gain*VAR[df$output]*VAR[df$Feature])
qc <- pipeline(res)
qc$method =  PKGMethod
qc.list[[PKGMethod]] <- qc
```



```{r,eval=recalc}
par(mfrow=c(1,3))

PKGMethod <- 'bnlearn.pc'
PKG = strsplit(PKGMethod,'\\.')[[1]][1]
library(PKG,character.only = T)

lst <- list()
PKGMethod <- 'bnlearn.bde'
res <- bnlearn::hc(as.data.frame(expr.dat.bin),score='bde')
qc <- performance.pairs(
  genes,
  res$arcs,
  true.pairs
)
qc$tab
qc$method =  PKGMethod
qc.list[[PKGMethod]]<-qc
lst <- c(lst,list(qc))



PKGMethod <- 'bnlearn.pc'
res <- bnlearn::pc.stable(expr.dat.bin)
qc <- performance.pairs(
  genes,
  res$arcs,
  true.pairs
)
qc$tab
qc$method =  PKGMethod
qc.list[[PKGMethod]]<-qc
lst <- c(lst,list(qc))
```

```{r,eval=recalc}
save(qc.list,file='qc-cache.rdf5')
```

```{r,eval = T}
load('qc-cache.rdf5')
```

```{r, fig.cap=cap,fig.height=4}
# plot(0,0,type='n',xlim=c(0,1),ylim=c(0,1))
cap <- ''

suppressMessages(
  {
    
    out <- lapply(qc.list,function(x)if(is.null(x$qc.dat$PR)){list()}else{cbind(x$qc.dat,method=x$method)})
  
  df <- combine_args(rbind)(out)
  
  p1 <- ggplot(df) + geom_line(aes(x=RC,y=PR,color=method)) + theme(legend.position = 'bottom') +
    ylab('Precision') + xlab('Recall')
  p2 <- ggplot(df) + geom_line(aes(x=1-SP,y=RC,color=method)) + 
    geom_abline(intercept = 0,slope=1,linetype=2) +xlim(0,1)+ylim(0,1) +
    theme(legend.position = 'none') +
    ylab('True postiive rate (Recall)') + xlab('False positive rate')
  
  g_legend<-function(a.gplot){
    require(gridExtra)
    ## source https://stackoverflow.com/questions/13649473/add-a-common-legend-for-combined-ggplots
    tmp <- ggplot_gtable(ggplot_build(a.gplot))
    leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
    legend <- tmp$grobs[[leg]]
    return(legend)}
  
  mylegend<-g_legend(p1)
  

  })


```

```{r perf-gnw-compare,fig.height=4,fig.cap=cap}
cap <- 'Comparing performances of different methods on the provided GNW sample'
p3 <- grid.arrange(arrangeGrob(p1 + theme(legend.position="none"),
                         p2 + theme(legend.position="none"),
                         nrow=1),
             mylegend, nrow=2,heights=c(10, 2))
# print(p3)
ggsave(p3,filename = 'pref-gnw-compare.png',width=8,height=4)->rub
```

```{r perf-gnw,eval= T}
out <- lapply(qc.list,function(x)if(is.null(x$qc.dat$PR)){list()}else{
  t(cbind(x$globals))->y;rownames(y)<-x$method;y})
df <- combine_args(rbind)(out)
df <- as.data.frame(df)
df <- unlist.df(df)
# rownames(odf) <- rownames(df)
df <- df[order(df$AUPR,decreasing = T),]

knitr::kable(df,row.names = T,
              caption='Performance statistics on the GNW dataset')
```

```{r one-shot,eval=T}
tb <- rbind_list(qc.list[c('bnlearn.pc',
                           'bnlearn.bde')])
tb <- tb[, !(colnames(tb) %in% c('tab')) ]

knitr::kable(tb,
             caption = 'Comparision of one-shot algorithms that output binary weights'
            )
```

## One-shot algorithms

pc.stable and hc(score='bde') was run on the GNW dataset but the precision was not so exciting (table \ref{tab:one-shot}). However, notice pc.stable is giving a higher precision, potentially warranting a bootstrap checking in the future.


## A homemade regressional predictor with xgboost(@xgboost)

GENIE3 (@GENIE3) uses random forest to fit the decision tree and then extract the importance. But here I will be fitting the trees with "xgboost" instead because of its simple interface and famous efficacy (@xgboost). Specifically, I fitted a decision tree to the function $x_j = f(\vec{x}_{-j})$, where $x_j$ is expression of the chosen gene and $x_{-j}$ is that of the rest genes. I used the Least sqaure loss ("reg:linear") for the fitting.

By the magic of xgboost, an importance vector is obatined ($p_j^i$ indicates relative importance of gene $i$ in predicting gene $j$, with $\sum_{i}p_j^i=1$). I intuitively intepret it as the ratio of variance explained by splitting on gene $i$ (though without firm matheamtical checking). Hence the importance vector is then scaled by the variance of the predictee ($P_{ij}=p_j^i\cdot Var(p_j)$) and then pooled together as the final confidence score for the connection between gene i and gene j, with a higher score indicating a higher likelihood of interaction. 

## Comparison of algorithms

A preliminary comparision is done between GeneNet, naive-spearman-predictor, bootstraped discrete bayesian net (bnlearn::boot.strength), GENIE3, homemade-xgboost (Gxgb) and MRnet. (figure \ref{fig:perf-gnw-compare}, table \ref{tab:perf-gnw}). It is conceivable that GENIE3 gave the best AUPR (~0.377), but discrete bayesian net also gave a very robust prediction. 


## Assessing the performance of algorithms with bootstrapped subnetworks

Note all predictions are very far from perfect (AUPR=1.00), which leads to the evaluation part. In short, each algorithm has pros and cons. Regressional predictors usually has very poor intepretability, and there is no mathematical framework that would guarantee its convergence to the true network. With the discrete bayesian networks, the discretisation is a somewhat arbitrary process that might introduce artefact to the result, as well as choosing a suitable imaginary sample size (ISS) for the network (here I will stick to ISS=1 for simplicity).  

Furthermore, a big headache for evaluating any algorithm is its performance will depend on the dataset. A most trivial example consider comparing the prediction result on the 40-gene dataset and then on a random sub-network generated by performing a random walk on the original network in a post-hoc fashion (so that the sub-network has non-zero adjacency matrix due to connection between neighbors). The AUPR is chosen as the main statistics and recorded for each sub-network to produce a bootstrapped distribution (figure \ref{fig:assignment-subnet}). This procedure is performed for GENIE3 and bnlearn because they gave the best performance in preliminary benchmarking (table \ref{tab:perf-gnw}). Gxgb was not included due to its slowness (further optimisation is required). 

```{r assignment-subnet,eval= T,fig.height=3,fig.cap=cap}
cap='Comparing "GENIE3" and "bnlearn" on the bootstrapped subnets of the provided GeneNetWeaver simulation'
load('assignment-subnet.rdf5')
suppressMessages({p <- plot.lst(lst)})
ggsave(p,filename = 'assignment-subnet.png',width=8,height=4)
```


```{r ecoli-subnet,eval= T,fig.height=3,fig.cap=cap}
cap='Comparing "GENIE3" and "bnlearn" on the bootstrapped subnets of DREAM5 network 3 (E. coli)'
load('e-coli-subnet.rdf5')
suppressMessages({p <- plot.lst(lst)})
ggsave(p,filename = 'ecoli-subnet.png',width=8,height=4)
```

The same bootstrapping was performed on the original DREAM5 training data for network 3  (the E. coli network, 4511 genes, figure \ref{fig:ecoli-subnet}). It can be seen that bootstrapped bnlearn performs similarly to the GENIE3, while taking a much shorter time to run (both measured using 16 cores). Although this may be due to the small size of the sub-network, (10 nodes), it is possible that GENIE3 is performing redundant computation than actual required by the inference.  It is also observed that both algorithms performs consistently poorer on the DREAM5 datasets. 

```{r,eval= F,fig.height=3}
par(mfrow=c(1,3))
lapply(qc.list,
  function(qc.meta){
   if(length(qc.meta)==5){
     # par(height=)
     diagnose(qc.meta)
     mtext(qc.meta$method,line=0)
   }else{
   }
   NULL
  }) ->rub
```




