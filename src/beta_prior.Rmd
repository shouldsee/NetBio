---
bibliography: citation.bib
csl: elsevier-vancouver.csl
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::pdf_document2:
    includes:
      in_header: Templates/preamble.tex
    keep_tex: yes
    number_sections: yes
    toc: yes
  html_document: default
---

```{r,echo = F,results='hide'}
recalc = F

knitr::opts_chunk$set(
  echo = F
 # ,results = 'hide' ##### Will hide tables
 ,cache = F
 ,eval = T
 ,eval.after = c('fig.cap','code')
)
suppressPackageStartupMessages({
  
  source('fitting_beta.R')
#   source('dream5.R')
#   library(ggplot2)
})
# if(recalc){
#   clu = parallel::makeCluster(16)
# }
options(digits=3,
        warn=-1)
```


## Finding priors for the beta distribution

Beta distribution is the natural prior for a binomial/bernoulli distribution. Considering distribution of a binomial variable $X\gvn\theta\sim Binom(\theta)$, in order to make its marginalisd distribution $P(X) = \int P(X\gvn\theta)P(\theta).d\theta$ analytically tractable, one of the choice is to assume $\theta\sim Beta(M,\alpha)$, so that:

$$
\begin{aligned}
P(\theta) &=\frac{x^{\alpha - 1}(1-x)^{M-\alpha-1}}{B(\alpha,M-\alpha)}
\\
E(\theta) &={\alpha\over M }
\end{aligned}
$$

### Constraint 1:

$$
\begin{aligned}
P(\theta\le 0.25) = P(\theta\ge 0.75) =  0.05 \\
P(\theta\le 0.75)=0.95
\end{aligned}
$$

Fitted result: $\theta\sim$ Beta(`r res1$par$a`, `r with(res1$par,m-a)` )

### Constraint 2


$$
\begin{aligned}
\text{argmax}_\theta(P(\theta))=0.4 \\
P(\theta\le 0.3) = 0.1
\end{aligned}
$$

Fitted result: $\theta\sim$ Beta(`r res2$par$a`, `r with(res2$par,m-a)` )


```{r constrained-beta,fig.cap=cap,fig.height=3,results='hide'}
cap <- 'The shape of the beta distribution constrianed under the respective conditions. Left: Constraint 1. Right: Constraint 2. (green: PDF, black: CDF)'
out <- res1
par(mfrow=c(1,2))
# par(mar=c(4.2,4.1,2.1,4.1))
par(mar=c(4.2,2.1,2.1,2.1))
preview(out$p,type='l'
        ,xlab = bquote(theta)
        ,ylab = bquote('P'~(theta))
                      )
abline(h = c(0.95,0.05),col=2,lty=2)
x = c(.25,.75)
y = out$p(x)
points(x,y)
text(x,y,sprintf('(%.3f,%.3f)',x,y),adj = -.15*c(-1,1))

options(digits = 3)
tl = with(data = out$par,bquote(M == .(m) ~","~ A == .(a) ~","~ loss==.(out$value) ))

title(tl)
grid()

out <- res2
preview(out$p,type='l'
        ,xlab = quote(theta)
        ,ylab = quote(P(Theta<theta))
                      )
abline(v = 0.4,col=2,lty=2)

par(new=T)
x = c(.3)
y = out$p(x)
points(x,y)
text(x,y,sprintf('(%.3f,%.3f)',x,y),adj = -.25*c(-3,2))

abline(h = y,col=2,lty=2)
tl = with(data = out$par,bquote(M == .(m) ~","~ A == .(a) ~","~ loss==.(out$value) ))
title(tl)

par(new=T)

preview(out$d,type='l',ylab='',xlab='',axes = F,
        col=3)
axis(side = 4)
mtext(side= 4,line = 1,quote(P(Theta=theta)))
grid()
```


```{r,fig.show='hide'}

source('bayes_infer.R')

```

```{r bayes,fig.cap = cap,fig.height=3}
cap = paste('Inference of posterior distribution on parameter $\\theta$ given mutatble sequence:', obsS)

ob <- q2infer

plot_bayes(ob)

# do.call(lines,c(dat,col=2))
# preview(post,type='l')


# outcounts
```

```{r,fig.show='hide'}
#### Recursively fitting the posterior
post <- ob$Fparam$post
L = ob$Fparam$L
preview(L,silent = F)
par(new = T)
preview(post,silent = F)
post <- bayes_infer(post,L)
combine_args(lines)(preview(post,silent = T))
post <- bayes_infer(post,L)
combine_args(lines)(preview(post,silent = T))
post <- bayes_infer(post,L)

```


### Basics for Bayesian inference \label{sec:bayes-basics}

$$
\begin{aligned}
\text{likelihood}&: f(\theta) = P(x \gvn \theta) \\
\text{prior}&: f(\theta) = P(\theta) \\
\text{posterior}&: f(\theta)  = P(\theta \gvn x) = \frac{  P(x\gvn \theta) P(\theta)  }{P(x) } \\
\text{marginal likelihood} &: P(x) =  \int P(x\gvn \theta) P(\theta).d\theta
\end{aligned}
$$

The observed toss sequence is: `r obsS`

Assume each coin toss is independent from each other, the likelihood of an observed sequence is only dependent on the total number of heads and not the sequence it occurred in. Denoting the coin toss as a sequence $\{x_i\}$ where $x_i \in \{0,1\}$, we have

$$
\begin{aligned}
\text{\#head} = \indicator\{x_i=1\} \\
\text{\#head} \sim Binom(|\{x_i\}|,\theta)
\end{aligned}
$$
Combining with the prior $\theta\sim$ Beta(`r res2$par$a`, `r with(res2$par,m-a)` ), I calculated the marginal likelihood numerically to be $P(x)=$ `r q2infer$marg$obs` and derived the posterior distribution accordingly (see figure \ref{fig:bayes}). MLE is obtained at $\hat{\theta}=$ `r xs=seq(0.4,0.6,length.out=500);mle=xs[which.max(ob$Fparam$post(xs))];mle`

