\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{}
  \pretitle{\vspace{\droptitle}}
  \posttitle{}
  \author{}
  \preauthor{}\postauthor{}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{15 March, 2018}

\usepackage{bbm} 
\newcommand{\D}{\text{d}}
\newcommand{\indicator}{\mathbbm{1}}
\newcommand{\hamm}{d_h}
\newcommand{\symhamm}{d_{sh}}
\newcommand{\hz}{\text{Hz}}
\newcommand{\tr}{\text{tr}}
\newcommand{\Iext}{{I_e/A}}
\newcommand\gvn[1][]{\:#1\vert\:}
\usepackage{afterpage}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{booktabs} 
\usepackage{placeins}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\begin{verbatim}
## All packages requirements are met: "doSNOW","doParallel","doMPI"
\end{verbatim}

\subsection{Comparing DREAM5
competitors}\label{comparing-dream5-competitors}

\subsubsection{Method}\label{method}

\paragraph{Data}\label{data}

GNW network: I used the pre-simulated data ``medium\_network.rda'' that
included the expression levels of 40 genes along with the underlying
true network to test the performance of various algorithms. This dataset
is generated by GeneNetWeaver ({[}1{]}) hence called GNW dataset.

DREAM5 dataset: The E. coli network in the Dream5 competition has proved
to be fittable and hence used here to evaluate the consistence of
algorithms.

\paragraph{Formalism}\label{formalism}

I assumed the interaction matrix \(A_{ij}\) is symmetric and binary, and
\(A_{ij}=1\) if there is interaction between gene i and gene j,
otherwise \(A_{ij}=0\). The problem then reduce to a binary
classification (existence of an undirected edge) and standard confusion
matrix may be constructed to evaluate the performance. Specifically,
area-under-precision-recall-curve (AUPR) and area-under-the-ROC (AROC)
will be plotted for comparision, along with a list of F1 score.

\begin{figure}
\centering
\includegraphics{dream5_files/figure-latex/perf-gnw-compare-1.pdf}
\caption{\label{fig:perf-gnw-compare}Comparing performances of different
methods on the provided GNW sample}
\end{figure}

\begin{verbatim}
## TableGrob (2 x 1) "arrange": 2 grobs
##   z     cells    name              grob
## 1 1 (1-1,1-1) arrange   gtable[arrange]
## 2 2 (2-2,1-1) arrange gtable[guide-box]
\end{verbatim}

\begin{table}

\caption{\label{tab:perf-gnw}Performance statistics on the GNW dataset}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & AROC & AUPR & F1\\
\hline
GENIE3 & 0.719 & 0.322 & 0.397\\
\hline
xgboost.Gxgb & 0.754 & 0.283 & 0.377\\
\hline
bnlearn.bde.bootstrap & 0.751 & 0.282 & 0.380\\
\hline
minet.mrnet & 0.746 & 0.268 & 0.386\\
\hline
naive.spearman & 0.662 & 0.159 & 0.278\\
\hline
GeneNet & 0.604 & 0.139 & 0.195\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:one-shot}Comparision of one-shot algorithms that output binary weights}
\centering
\begin{tabular}[t]{r|r|r|r|r|r|r|r|r|l}
\hline
TP & FP & TN & FN & PR & RC & SP & NPN & F1 & method\\
\hline
8 & 14 & 705 & 53 & 0.364 & 0.131 & 0.981 & 0.930 & 0.193 & bnlearn.pc\\
\hline
11 & 25 & 694 & 50 & 0.306 & 0.180 & 0.965 & 0.933 & 0.227 & bnlearn.bde\\
\hline
\end{tabular}
\end{table}

\subsection{One-shot algorithms}\label{one-shot-algorithms}

pc.stable and hc(score=`bde') was run on the GNW dataset but the
precision was not so exciting (table \ref{tab:one-shot}). However,
notice pc.stable is giving a higher precision, potentially warranting a
bootstrap checking in the future.

\subsection{A homemade regressional predictor with
xgboost({[}2{]})}\label{a-homemade-regressional-predictor-with-xgboostxgboost}

GENIE3 ({[}3{]}) uses random forest to fit the decision tree and then
extract the importance. But here I will be fitting the trees with
``xgboost'' instead because of its simple interface and famous efficacy
({[}2{]}). Specifically, I fitted a decision tree to the function
\(x_j = f(\vec{x}_{-j})\), where \(x_j\) is expression of the chosen
gene and \(x_{-j}\) is that of the rest genes. I used the Least sqaure
loss (``reg:linear'') for the fitting.

By the magic of xgboost, an importance vector is obatined (\(p_j^i\)
indicates relative importance of gene \(i\) in predicting gene \(j\),
with \(\sum_{i}p_j^i=1\)). I intuitively intepret it as the ratio of
variance explained by splitting on gene \(i\) (though without firm
matheamtical checking). Hence the importance vector is then scaled by
the variance of the predictee (\(P_{ij}=p_j^i\cdot Var(p_j)\)) and then
pooled together as the final confidence score for the connection between
gene i and gene j, with a higher score indicating a higher likelihood of
interaction.

\subsection{Comparison of algorithms}\label{comparison-of-algorithms}

A preliminary comparision is done between GeneNet,
naive-spearman-predictor, bootstraped discrete bayesian net
(bnlearn::boot.strength), GENIE3, homemade-xgboost (Gxgb) and MRnet.
(figure \ref{fig:perf-gnw-compare}, table \ref{tab:perf-gnw}). It is
conceivable that GENIE3 gave the best AUPR (\textasciitilde{}0.377), but
discrete bayesian net also gave a very robust prediction.

\subsection{Assessing the performance of algorithms with bootstrapped
subnetworks}\label{assessing-the-performance-of-algorithms-with-bootstrapped-subnetworks}

Note all predictions are very far from perfect (AUPR=1.00), which leads
to the evaluation part. In short, each algorithm has pros and cons.
Regressional predictors usually has very poor intepretability, and there
is no mathematical framework that would guarantee its convergence to the
true network. With the discrete bayesian networks, the discretisation is
a somewhat arbitrary process that might introduce artefact to the
result, as well as choosing a suitable imaginary sample size (ISS) for
the network (here I will stick to ISS=1 for simplicity).

Furthermore, a big headache for evaluating any algorithm is its
performance will depend on the dataset. A most trivial example consider
comparing the prediction result on the 40-gene dataset and then on a
random sub-network generated by performing a random walk on the original
network in a post-hoc fashion (so that the sub-network has non-zero
adjacency matrix due to connection between neighbors). The AUPR is
chosen as the main statistics and recorded for each sub-network to
produce a bootstrapped distribution (figure
\ref{fig:assignment-subnet}). This procedure is performed for GENIE3 and
bnlearn because they gave the best performance in preliminary
benchmarking (table \ref{tab:perf-gnw}). Gxgb was not included due to
its slowness (further optimisation is required).

\begin{figure}
\centering
\includegraphics{dream5_files/figure-latex/assignment-subnet-1.pdf}
\caption{\label{fig:assignment-subnet}Comparing ``GENIE3'' and ``bnlearn''
on the bootstrapped subnets of the provided GeneNetWeaver simulation}
\end{figure}

\begin{figure}
\centering
\includegraphics{dream5_files/figure-latex/ecoli-subnet-1.pdf}
\caption{\label{fig:ecoli-subnet}Comparing ``GENIE3'' and ``bnlearn'' on the
bootstrapped subnets of DREAM5 network 3 (E. coli)}
\end{figure}

The same bootstrapping was performed on the original DREAM5 training
data for network 3 (the E. coli network, 4511 genes, figure
\ref{fig:ecoli-subnet}). It can be seen that bootstrapped bnlearn
performs similarly to the GENIE3, while taking a much shorter time to
run (both measured using 16 cores). Although this may be due to the
small size of the sub-network, (10 nodes), it is possible that GENIE3 is
performing redundant computation than actual required by the inference.
It is also observed that both algorithms performs consistently poorer on
the DREAM5 datasets.

\hypertarget{refs}{}
\hypertarget{ref-gnw}{}
{[}1{]} Schaffter T, Marbach D, Floreano D. GeneNetWeaver: in silico
benchmark generation and performance profiling of network inference
methods. Bioinformatics (Oxford, England) 2011;27:2263--70.
doi:\href{https://doi.org/10.1093/bioinformatics/btr373}{10.1093/bioinformatics/btr373}.

\hypertarget{ref-xgboost}{}
{[}2{]} Chen T, Guestrin C. XGBoost: A scalable tree boosting system.
CoRR 2016;abs/1603.02754.

\hypertarget{ref-GENIE3}{}
{[}3{]} Schaffter T, Marbach D, Floreano D. GeneNetWeaver: in silico
benchmark generation and performance profiling of network inference
methods. Bioinformatics (Oxford, England) 2011;27:2263--70.
doi:\href{https://doi.org/10.1093/bioinformatics/btr373}{10.1093/bioinformatics/btr373}.


\end{document}
